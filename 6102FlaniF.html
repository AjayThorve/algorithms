<html>
    <head>
        <link href="style.css" rel="stylesheet" type="text/css"/>
        <title>
            Design and Analysis of Algorithms Final
        </title>
    </head>

    <body>
        <h1>
            Design and Analysis of Algorithms Final
        </h1>
        <h2>
            Problems
        </h2>

        <ol>
            <li>Let us say we have an algorithm that runs 
            in 10n<sup>2</sup> + 10 time.
                We want to show its order of complexity is
                &theta;(n<sup>2</sup>). Give
                an example of a set of k<sub>1</sub>, k<sub>2</sub>,
                and n<sub>0</sub> that would do this.
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
            <li>Imagine you have an 8-sided die, numbered 1 through 8. Use an
                indicator random variable to show how many 8s we expect to get
                in 100 rolls.
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
            <li>Use an indicator random variable to show how many numbers we
                have to pick from a set of <em>n</em> numbers before we are
                more likely than not to have drawn the same number twice.
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
            <li>Describe why the fractional knapsack problem can be solved with
                a greedy algorithm, but the whole unit problem cannot.
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
            <li>Prove that a greedy algorithm can correctly find a minimum
                spanning tree.
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
                <br> <br> <br> <br> <br>
        </ol>

        <hr>
        <h3>
            Multiple Choice
        </h3>
        <ol>
            <br>
            <li>Compared to chaining, open addressing has the downside that:
                <ol type="a">
                    <li>It is very slow.
                    <li>The hash table can fill up. *
                    <li>It can't resolve collisions.
                    <li>The hash table takes a huge amount of space.
                </ol>

            <br>
            <li>We have one algorithm for processing customer records
                with run time of O(n), and another with
                run time of O(lg n) + 1000. In what 
                circumstances <i>might</i> we want
                to choose the O(n) algorithm?
                <ol type="a">
                    <li>No circumstances.
                    <li>If our programmers are really bad.
                    <li>We believe our program will 
                        always be dealing with a
                        number of records less than one thousand. *
                    <li>If <em>n</em> is very large.
                </ol>

            <br>
            <li>Under what circumstances might we 
                most reasonably expect to
                receive worst-case input to our binary search tree?
                <ol type="a">
                    <li>We are loading the tree from an 
                        existing data store which is sorted. *
                    <li>We insert data into the tree 
                        as it comes along to us.
                    <li>The data has been randomly shuffled.
                    <li>All of the above.
                </ol>
            <br>

            <li>When we randomize an algorithm, we then speak of its
                <ol type="a">
                    <li>necessary running time.
                    <li>best-case running time.
                    <li>expected running time. *
                    <li>unbounded running time.
                </ol>
            <br>

            <li>The linearity of expectations means that, for two events X
                and Y, the expectation that either event will occur equals
                <ol type="a">
                    <li>the product of the expectation that X will occur
                        and the expectation that Y will occur
                    <li>the difference of the expectation that X will occur
                        and the expectation that Y will occur
                    <li>the square of the expectation that X will occur
                        and the expectation that Y will occur
                    <li>the sum of the expectation that X will occur and
                        the expectation that Y will occur *
                </ol>
            <br>

            <li>The worst-case running time of a search on a binary search
                tree is
                <ol type="a">
                    <li>&theta;(n<sup>2</sup>)
                    <li>&theta;(n) *
                    <li>&theta;(lg n)
                    <li>&theta;(1)
                </ol>

            <br>
            <li>If we have a binary search tree of height <em>h</em>, then
                all searching functions such as min, max, and successor
                will run in
                <ol type="a">
                    <li>O(h) time. *
                    <li>O(h<sup>2</sup>) time.
                    <li>O(ln h) time.
                    <li>O(lg h) time.
                </ol>

            <br>
            <li>According to the binary search tree property
                <ol type="a">
                    <li>for all nodes higher in the tree than x, 
                        x.key &le; y.key.
                    <li>for all nodes in the right subtree of x,
                        x.key &le; y.key. *
                    <li>for all nodes lower in the tree than x, 
                        x.key &le; y.key.
                    <li>all nodes in the right subtree of x will
                        be the successors of x.
                </ol>

            <br>
            <li>The expected value of a real variable X is:
                <ol type="a">
                    <li>is the weighted sum of all its values, weighted
                        according to the probability that they happen. *
                    <li>the single value that occurs most often.
                    <li>the value in the middle of its probability
                        distribution.
                    <li>all of the above, as they are equivalent.
                </ol>

            <br>
            <li>&Theta;-notation applied to a function implies
                it is
                <ol type="a">
                    <li>a function we know little about.
                    <li>asymptotically bound from above and below. *
                    <li>asymptotically bound from below only.
                    <li>asymptotically bound from above only.
                </ol>

            <br>
            <li>For large n, an algorithm will run the <em>slowest</em>
                 if it time complexity is:
                <ol type="a">
                    <li>O(20n lg n)
                    <li>O(n!) *
                    <li>O(n<sup>3</sup>)
                    <li>O(2<sup>n</sup>)
                </ol>

            <br>
            <li>An algorithm is most like:
                <ol type="a">
                    <li>a carefully designed recipe. *
                    <li>a car engine.
                    <li>an organic molecule.
                    <li>a literary work.
                </ol>

            <br>
            <li>O-notation applied to a function implies
                <ol type="a">
                    <li>it is a function we know little about.
                    <li>it is asymptotically bound from above and below.
                    <li>only that it is asymptotically bound from below.
                    <li>only that it is asymptotically bound from above. *
                </ol>

            <br>
            <li>For large n, an algorithm will run the <em>fastest</em>
                 if it time complexity is:
                <ol type="a">
                    <li>O((1.1)<sup>n</sup>)
                    <li>O(n!)
                    <li>O(n<sup>2</sup>) *
                    <li>O(n<sup>3</sup>)
                </ol>

            <br>
            <li>Besides running time, we can also measure algorithm performance
                by:
                <ol type="a">
                    <li>disk usage
                    <li>memory usage
                    <li>power consumed
                    <li>all of the above *
                </ol>

            <br>
            <li>If f(n) = &omega;(g(n)), that means
                <ol type="a">
                    <li>g dominates f asymptotically
                    <li>f is bounded below by g asymptotically
                    <li>f is bounded above by g asymptotically
                    <li>f dominates g asymptotically *
                </ol>

            <br>
            <li>Which of these functions grows most <em>slowly</em>?
                <ol type="a">
                    <li>lg n
                    <li>lg* n *
                    <li>ln n
                    <li>log<sub>10</sub> n
                </ol>

            <br>
            <li>&Omega;-notation applied to a function implies 
                <ol type="a">
                    <li>it is a function we know little about.
                    <li>it is asymptotically bound from above and below.
                    <li>only that it is asymptotically bound from below. *
                    <li>only that it is asymptotically bound from above.
                </ol>

            <br>
            <li>In algorithm analysis, we usually analyze algorithms
                in terms of
                <ol type="a">
                    <li>actual running time in nanoseconds.
                    <li>the number of disk operations.
                    <li>the number of basic operations. *
                    <li>CPU time used.
                </ol>

            <br>
            <li>In algorithm analysis, we usually look at
                <ol type="a">
                    <li>worst-case performance. *
                    <li>average-case performance.
                    <li>best-case performance.
                    <li>median-case performance.
                </ol>

            <br>
            <li>The chief advantage of a doubly-linked list over a linked list is
                <ol type="a">
                    <li>we can walk the list in either direction. *
                    <li>inserts are faster.
                    <li>deletes are faster.
                    <li>less memory is used.
                </ol>

            <br>
            <li>A good use of direct dictionaries would be
                <ol type="a">
                    <li>memoization of a numeric algorithm.
                    <li>coding the Sieve of Eratosthenes.
                    <li>marking zipcodes seen in a mailing application.
                    <li>all of the above. *
                </ol>

            <br>
            <li>The worst case of hashing with chaining occurs when
                <ol type="a">
                    <li>the input comes in sorted already.
                    <li>all inputs hash to different values.
                    <li>the input is purely random.
                    <li>all inputs hash to the same value. *
                </ol>

            <br>
            <li>The "clustering" problem with open addressing with linear
                probing means that
                <ol type="a">
                    <li>"clusters" of keys will all hash 
                        to the same value.
                    <li>"clusters" of keys will build up in a linked list.
                    <li>the hash table clusters around its central value.
                    <li>once inputs start to cluster in one area of the
                        hash table, they will become more likely 
                        to do so. *
                </ol>
            <br>

            <li>A random variable is actually
                <ol type="a">
                    <li>the mean of all possible values.
                    <li>an algorithm.
                    <li>a precise real number value.
                    <li>a function. *
                </ol>
            <br>

            <li>We can sometimes be loose with our analysis of
                divide-and-conquer algorithms, 
                in that we might omit details like
                <ol type="a">
                    <li>recursions and function calls.
                    <li>exponential factors.
                    <li>floors, ceilings, and boundary conditions.
                    <li>any n raised to a power less than 3.
                </ol>
            <br>

            <li>In the substitution method for solving recurrences,
                we
                <ol type="a">
                    <li>substitute a difference recurrence for
                        the one characterizing our algorithm.
                    <li>guess a solution and then 
                        use induction to prove it.
                    <li>substitute a polynomial factor for 
                        an exponential one.
                    <li>substitute an n<sup>2</sup> wherever
                        we see a 2<sup>n</sup> 
                </ol>
            <br>

            <li>Consider the following recursion tree.
                <br>
                What closed form best characterizes this tree?
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>Consider this recurrence:
                    <br>
                    <img src="graphics/RecEq6.gif">
                    <br>
                What is the solution to this recurrence?
                <ol type="a">
                    <li>T(n) = &Theta;(n<sup>3</sup>) 
                    <li>T(n) = &Theta;(n<sup>2</sup>) 
                    <li>T(n) = &Theta;(n<sup>8</sup>) 
                    <li>T(n) = &Theta;(n<sup>8/3</sup>) 
                </ol>
            <br>

            <li>If you have available a simple algorithm with some 
                acceptable run time, and a more complex algorithm 
                with slightly faster run time, you should
                <ol type="a">
                    <li>always choose the faster algorithm.
                    <li>always choose the simpler algorithm.
                    <li>consider how big your input is likely
                        to be before choosing either algorithm.
                    <li>ask your boss what to do.
                </ol>
            <br>

            <li>In graph theory, a tree is
                <ol type="a">
                    <li>a forest with no edges.
                    <li>a cyclical graph with no forests.
                    <li>any walk on a graph.
                    <li>a connected graph with no cycles.
                </ol>
            <br>

            <li>Graph theory is useful for
                <ol type="a">
                    <li>task management.
                    <li>map coloring.
                    <li>cellular telephone networks.
                    <li>all of the above.
                </ol>
            <br>

            <li>Consider the following graph:
                <br>
                Which of the following would be an adjacency list
                representation of this graph?
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>Consider the following graph:
                <br>
                Which of the following would be an adjacency matrix
                representation of this graph?
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>An advantage of the adjacency matrix representation
                over the adjacency list representation is
                <ol type="a">
                    <li>it is smaller. 
                    <li>it allows quicker searches as to whether some
                        edge exists.
                    <li>it can represent weighted graphs.
                    <li>it can represent directed graphs.
                </ol>
            <br>

            <li>Which of the following matrices represents a directed
                graph with edges from 
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>Breadth-first search question.
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>Depth-first search question.
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>Strongly connected components of a graph are
                <ol type="a">
                    <li>components of a directed graph that can
                        each be reached from each other.
                    <li>components in a weighted graph where the 
                        connections have the highest weights.
                    <li>components of a spanning tree.
                    <li>
                </ol>
            <br>

            <li>Dynamic programming is preferred to simple recursive 
                solutions when
                <ol type="a">
                    <li>the recursive solution is hard to grasp.
                    <li>all of the sub-problems are completely
                        independent of each other.
                    <li>the recursion goes beyond 4 levels.
                    <li>the same sub-problems must be solved multiple
                        times.
                </ol>
            <br>

            <li>Often, a good alternative to dynamic programming is
                <ol type="a">
                    <li>a simple recursive solution.
                    <li>a recursive solution with memoization.
                    <li>a brute-force solution.
                    <li>all of the above.
                </ol>
            <br>

            <li>A greedy algorithm is appropriate when
                <ol type="a">
                    <li>we need to consider the global situation
                        when making any choice.
                    <li>there is extensive over-lap among sub-problems.
                    <li>only the locally best choice matters.
                    <li>greed is the goal of the algorithm choice.
                </ol>
            <br>

            <li>We can use a greedy algorithm to solve
                <ol type="a">
                    <li>a rod-cutting problem.
                    <li>a minimum-spanning-tree problem.
                    <li>a matrix parenthisization problem.
                    <li>a shortest-path problem.
                </ol>
            <br>

            <li>For a greedy algorithm to work, the optimal choice
                <ol type="a">
                    <li>must depend on many over-lapping sub-problems.
                    <li>must not depend on any future choices.
                    <li>must be dependent on a global optimal.
                    <li>must only be available after considering 
                        all sub-problems.
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

            <li>
                <ol type="a">
                    <li>
                    <li>
                    <li>
                    <li>
                </ol>
            <br>

        </ol>
    </body>
</html>
