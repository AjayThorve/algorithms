{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 1 (Sept 12, 2016): Introduction and preliminaries*\n",
    "\t\n",
    "    •\tIntroduction. \n",
    "        Syllabus: Logistics, grading, exams, homeworks, cheating.\n",
    "\t\n",
    "    •\tWe will study algorithms. What's an algorithm: a carefully written recipe. We need to agree what steps are allowed in a recipe. We need to agree what problem the recipe is solving, ahead of time. \n",
    "\t\n",
    "    •\tThings to keep in mind about algorithms (when analyzing and/or designing them):\n",
    "\t\n",
    "        ◦\tTermination: On any legal (meaning, consistent with algorithm specification) input, the procedure you are describing should always eventually stop, or terminate. (In this course we will not talk about algorithms that are intended to run \"forever,\" such as the scheduler in an operating system.) \n",
    "\t\n",
    "        ◦\tCorrectness: On any legal input, provided the procedure has terminated, the result that it produces has to be correct. Not sometimes correct, not mostly correct, but always correct. \n",
    "\t\n",
    "        ◦\tAnd finally, performance: You can measure performance in many different ways. The course will focus on the running time, but there are many others: space (memory use), disk use, amount of disk-memory communication, number of processors in a parallel program, total amount of work in a parallel program, number of cores in a multi-core program, total amount of communication in a distributed program, power consumed in an algorithm tuned to low-power devices and many many others. \n",
    "\t\n",
    "    •\tSo in short, when you describe a recipe for doing something, make sure it always stops, make sure it always produces correct answers, and only after that worry about how efficient it is. (Depending on the algorithm, some or all of these are very easy, and some may not be obvious at all. We will see examples later.) \n",
    "\t\n",
    "    •\tFocusing on running time, we surely do not want to talk about actual time in nanoseconds, as that depends on too many external things: what other processes are running, details of the hardware, compiler switches, etc, etc. So instead we come up with a very primitive abstract machine (RAM=the Random Access Machine: CPU + directly addressable memory) which we analyze instead of a real computer. Then primitive operations are memory reads/writes and operations in the CPU, such as address arithmetic, additions, multiplications, divisions, etc. Our idea of \"running time\" is then simply the number of operations performed when the program runs. \n",
    "\t\n",
    "    •\tMore trouble: even for inputs of a fixed size (saying, your favorite program sorting 10 numbers), different specific inputs will produce different performance. For example, a clever algorithm may notice that the input is already sorted and not try sorting it again. So, we distinguish between best- and worst-case performance (minimum and maximum number of operations performed by the algorithm, over all possible legal inputs of a given size). (There is also the concept of \"average-case,\" but it is tricky, as it requires a definition of what average means: technically, you need to specify a distribution of the inputs. We will mostly stay away from average-case analysis and focus on the worst case; we will occasionally look at the best case too.) \n",
    "\t\n",
    "    •\tWhat we really want, is not the running time (as the number of operations, say, in the worst case), for a specific input size, but as a function of the input size, over all sizes. For example, it may be that a given sorting algorithm requires $$4n^2-4n+24$$ operations to sort $$n$$ items. \n",
    "\t\n",
    "    •\tIn order to make life easier, we will not be focusing on the exact value of such function, but on its asymptotic behavior.\n",
    "    \n",
    "        ◦\tbig-Oh ($$O$$)\n",
    "        \n",
    "        ![Image of Big-O](https://upload.wikimedia.org/wikipedia/commons/8/89/Big-O-notation.png)\n",
    "        \n",
    "        ◦\tbig-Theta ($$\\Theta$$)\n",
    "        ◦\tbig-Omega ($$\\Omega$$)\n",
    "        ◦\tlittle-oh ($$o$$)\n",
    "        ◦\tlittle-omega ($$\\omega$$).\n",
    "    \n",
    "    \n",
    "    You should review them in the book. It's a language I am going to use throughout the course. If you are not comfortable with it, you will be lost. \n",
    "\t    \n",
    "All the material covered can be found in the textbook. Please review the material on asymptotic notation very carefully, as it will be used as the language in algorithmic analysis throughout the course. Plan was:\n",
    "\t\n",
    "    •\tIntroduction. Logistics. Syllabus. \n",
    "\t\n",
    "    •\tWe study algorithms. Why? What is it? What will we study? \n",
    "\t•\tTermination. Correctness. Performance. \n",
    "\t\n",
    "    •\tHow to measure stuff. Levels of abstraction. Abstract machine. Abstract performance. \n",
    "\t\n",
    "    •\tWorst/best/average case. \n",
    "\t\n",
    "    •\tAsymptotics. Big-oh. Quick review. \n",
    "\t\n",
    "    •\tAn example of a toy algorithmic problem and how to solve it, analyze it etc etc. \n",
    "\t\n",
    "    •\t... I will surely run out of time long before I get here ... \n",
    "\n",
    "   \n",
    "   * These notes are based on Professor Boris Aronov's notes from previous semesters.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
